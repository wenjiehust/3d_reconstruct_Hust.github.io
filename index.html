<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>3D Scene Reconstruction and Rendering from Multiple  Images by wenjiehust</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">3D Scene Reconstruction and Rendering from Multiple  Images</h1>
      <h2 class="project-tagline">基于多幅图片的三维重建和渲染</h2>

    </section>

    <section class="main-content">
      <h1>
<a id="简介" class="anchor" href="#%E7%AE%80%E4%BB%8B" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>简介</h1>

<p>三维重建目前是一个热门研究话题，通过对某一个静态场景的各个角度拍摄图片或者视频，然后用这些采集的视觉信息来通过多个视图几何的思想来重建拍摄的静态场景。
主要应用方面有重建毁坏的文物建筑、医学重建、三维地图、增强现实等领域，在目前以及未来都有很大市场前景和学术研究价值。
我们的工作就是通过一组围绕某一个静态场景的图片，用多视图几何的思想通过算法来重建得到场景的三维信息。</p>
<p>下面是用一组图片进行三维重建得到三维场景的视频</p>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/jtyXkuNPpIg?list=PL949A9422C7FB609F" frameborder="0" allowfullscreen></iframe></p>
<h1>
<a id="工作进展" class="anchor" href="#%E5%B7%A5%E4%BD%9C%E8%BF%9B%E5%B1%95" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>工作进展</h1>

<p>
  <b><font size="5">更新时间：2016年6月16日</font></b><br>
在用<a href="http://ccwu.me/vsfm/">Wuchangchang</a>的方法得到稠密的三维点云之后，我们用MeshLab软件进行了后续的
<a href="http://www.aichengxu.com/view/2585829">泊松表面重建和纹理映射工作</a>，下面是我们在MeshLab上进行实验中的一些截图：
第一幅是初始加载的稀疏三维点云以及相机姿态，第二幅是剔除杂点之后的稠密三维点云，第三幅是进行泊松表面重建并剔除一些多余网格之后的网格图，
第四幅是进行纹理映射之后的图像。
</p>
<p><img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/chushidianyun.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/chushidianyun.JPG">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/choumi.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/choumi.JPG">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/bossion.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/bossion.JPG">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/texture.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/texture.JPG">

</p>
<p>
  <b><font size="5">更新时间：2016年6月13日</font></b><br>
参照<a href="http://ccwu.me/vsfm/">Wuchangchang</a>的想法，上周做了一下GPU版本
的<a href="http://www.cs.unc.edu/~ccwu/siftgpu/">SIFTGPU</a>的相关代码的调试和运行，并得到了一些结果。这周继续上周的工作，
首先看了<a href="http://grail.cs.washington.edu/projects/mcba/">Multicore Bundle Adjustment</a>相关的文章，了解了多核Bundle Adjustment
的基本算法流程，然后把<a href="http://ccwu.me/vsfm/">VisualSFM</a>在Windows下运行了一下，得到了一些实验结果如下所示，第一幅是初始加载
多幅图片之后的界面结果；第二幅是进行SIFT特征点提取并多核快速匹配之后其中某两幅图片的匹配点连线结果；第三幅是进行多核BA之后，重建得到
的稀疏三维点云和相机姿态；第四幅是进行CMVS稠密扩展之后的稠密三维点云。
</p>
<p><img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfm初始.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfm初始.JPG">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfmsiftmatch2.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfmsiftmatch2.JPG">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfmbundler.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfmbundler.JPG">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfmcmvs.JPG" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/vsfmcmvs.JPG">

</p>

<p><b><font size="5">更新时间：2016年6月6日</font></b><br>
经过一段时间的文献阅读和资料检索整理，我们参照<a href="http://ccwu.me/vsfm/">Wuchangchang</a>的想法，看了其中一系列的文章，首先着手做了一下GPU版本
的<a href="http://www.cs.unc.edu/~ccwu/siftgpu/">SIFTGPU</a>的相关代码的调试和运行，对SIFT有了一定的认识和理解。下面是代码调试的结果图,第一幅图是
标注的绿色标记点标注了提取的sift关键点的位置，第二幅是得到的sift关键点文件中的关键点的信息。
</p>
<p><img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/0.jpg" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/0.jpg">
<img src="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/1.jpg" alt="https://raw.githubusercontent.com/wenjiehust/hahahah.github.io/master/1.jpg">
</p>

<p><b><font size="5">更新时间：2016年5月22日</font></b><br>
目前我们的工作还处于初步的文献阅读和算法的了解层面，主要看了《Multi-camera scene reconstruction via graph cuts》这篇文章，
了解了场景重建的一些思想。下面是文章中的实验结果图：
<img src="https://c7.staticflickr.com/8/7402/27307487846_cb1075f8a8_z.jpg" alt="https://c7.staticflickr.com/8/7402/27307487846_cb1075f8a8_z.jpg"></p>

<h1>
<a id="小组成员及联系方式" class="anchor" href="#%E5%B0%8F%E7%BB%84%E6%88%90%E5%91%98%E5%8F%8A%E8%81%94%E7%B3%BB%E6%96%B9%E5%BC%8F" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>小组成员及联系方式：</h1>

<pre><code>* 黄文杰  h9220@qq.com
* 徐涛   1961231910@qq.com
* 魏凯   962904637@qq.com
</code></pre>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/wenjiehust/3d_reconstruct_Hust.github.io">3D Scene Reconstruction and Rendering from Multiple  Images</a> is maintained by <a href="https://github.com/wenjiehust">wenjiehust</a>.</span>

      </footer>

    </section>

  
  </body>
</html>
